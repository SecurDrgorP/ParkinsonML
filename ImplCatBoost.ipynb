{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of ***CatBoost***\n",
    "This notebook demonstrates the implementation of the **CatBoost** for Parkinson’s Disease detection using three feature processing methods:  \n",
    "1. All Features (`Toutes caractéristiques`).  \n",
    "2. Feature Selection (`La Sélection des caractéristiques`): ***Wrapper***, ***Embedding***, and ***ANOVA***.  \n",
    "3. Dimensionality Reduction (`Réduction des données`) using ***PCA***.  \n",
    "\n",
    "Each method is applied in two ways:  \n",
    "- Using standard libraries.  \n",
    "- Using custom implementations developed from scratch.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_auc_score\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from Models.catboost import CatBoostScratch\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing  \n",
    "- **Dataset**: Parkinson's Disease Classification (UCI Machine Learning Repository).  \n",
    "- **Steps**:  \n",
    "  1. Load and explore the dataset.  \n",
    "  2. Handle missing values.  \n",
    "  3. Normalize features using MinMaxScaler.  \n",
    "  4. Balance classes using SMOTE.  \n",
    "- Code snippets for preprocessing are shared below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "data = pd.read_csv(\"pd_speech_features.csv\", header=1)\n",
    "\n",
    "\n",
    "# Preprocess numeric columns\n",
    "def safe_convert_numeric(df):\n",
    "    numeric_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in numeric_columns:\n",
    "        try:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting column {col}: {e}\")\n",
    "    return df\n",
    "\n",
    "data = safe_convert_numeric(data)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Identify numeric columns\n",
    "numeric_columns = data.select_dtypes(include=['float64', 'int64']).columns[1:-1]\n",
    "data[numeric_columns] = scaler.fit_transform(data[numeric_columns])\n",
    "\n",
    "# Identify binary/categorical columns and other features\n",
    "binary_features = ['gender']  # Adjust if needed\n",
    "other_features = [col for col in data.columns if col not in binary_features + ['id', 'class']]\n",
    "\n",
    "# Aggregate features with mean and std\n",
    "aggregated_data = data.groupby('id')[other_features].agg(['mean', 'std']).reset_index()\n",
    "\n",
    "# Flatten multi-level column names\n",
    "aggregated_data.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in aggregated_data.columns]\n",
    "\n",
    "# Add binary features back as-is\n",
    "for feature in binary_features:\n",
    "    aggregated_data[feature] = data.groupby('id')[feature].first().values.astype(data[feature].dtype)\n",
    "\n",
    "# Add the target variable\n",
    "aggregated_data['class'] = data.groupby('id')['class'].first().values\n",
    "\n",
    "# Separate features and target\n",
    "X = aggregated_data.drop(['class', 'id_'], axis=1)\n",
    "y = aggregated_data['class']\n",
    "\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Method 1: Using All Features  \n",
    "This section applies the ***CatBoost*** on the dataset using all features without any feature selection.  \n",
    "\n",
    "### Implementation Approaches:  \n",
    "1. **Using Standard Libraries** (e.g., scikit-learn, CatBoost).  \n",
    "2. **Custom Implementation** of **CatBoost**  \n",
    "\n",
    "### Evaluation Metrics:  \n",
    "- Accuracy, Precision, Recall, F1-Score, ROC-AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train using catqboostClassifier\n",
    "\n",
    "cat_all = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    loss_function='Logloss',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_all.fit(X_train, y_train)\n",
    "\n",
    "y_pred_all_cat = cat_all.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classifier:\n",
      "Accuracy: 0.7843137254901961\n",
      "ROC AUC: 0.6275303643724697\n",
      "Confusion Matrix:\n",
      "[[ 4  9]\n",
      " [ 2 36]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.31      0.42        13\n",
      "           1       0.80      0.95      0.87        38\n",
      "\n",
      "    accuracy                           0.78        51\n",
      "   macro avg       0.73      0.63      0.64        51\n",
      "weighted avg       0.77      0.78      0.75        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print(\"CatBoost Classifier:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_all_cat)}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_all_cat)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_all_cat)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_all_cat)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix:\n",
      "[[ 6  7]\n",
      " [ 3 35]]\n",
      "\n",
      "Accuracy score:\n",
      "0.803921568627451\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.46      0.55        13\n",
      "           1       0.83      0.92      0.88        38\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.75      0.69      0.71        51\n",
      "weighted avg       0.79      0.80      0.79        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train using catboostscratch\n",
    "\n",
    "cat_scratch_all = CatBoostScratch(\n",
    "    n_estimators=500,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "\n",
    "cat_scratch_all.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_cat_scratch_selected = cat_scratch_all.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_cat_scratch_selected))\n",
    "\n",
    "print(\"\\nAccuracy score:\")\n",
    "print(accuracy_score(y_test, y_pred_cat_scratch_selected))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred_cat_scratch_selected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Method 2: Feature Selection  \n",
    "Features are selected using three techniques: Wrapper, Embedding, and ANOVA. The union of selected features is used for model training.  \n",
    "\n",
    "### Sub-methods:  \n",
    "1. **Wrapper Method**: Backward elimination.  \n",
    "2. **Embedding Method**: LassoCV.  \n",
    "3. **ANOVA**: Statistical test for feature importance.  \n",
    "\n",
    "### Evaluation Metrics:  \n",
    "- Accuracy, Precision, Recall, F1-Score, ROC-AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Features: ['tqwt_kurtosisValue_dec_36_mean', 'tqwt_TKEO_std_dec_11_mean', 'tqwt_stdValue_dec_19_std', 'tqwt_kurtosisValue_dec_18_mean', 'std_delta_delta_log_energy_mean', 'det_LT_entropy_log_6_coef_mean', 'det_LT_entropy_shannon_8_coef_mean', 'tqwt_skewnessValue_dec_30_std', 'tqwt_entropy_shannon_dec_20_std', 'tqwt_minValue_dec_11_mean', 'det_LT_entropy_log_4_coef_mean', 'tqwt_stdValue_dec_12_mean', 'tqwt_stdValue_dec_10_mean', 'det_LT_entropy_log_3_coef_std', 'mean_MFCC_6th_coef_mean', 'det_LT_entropy_log_3_coef_mean', 'det_LT_entropy_shannon_8_coef_std', 'tqwt_maxValue_dec_13_mean', 'std_6th_delta_delta_mean', 'tqwt_kurtosisValue_dec_26_mean', 'tqwt_kurtosisValue_dec_27_mean', 'gender', 'tqwt_stdValue_dec_11_mean', 'tqwt_kurtosisValue_dec_34_mean', 'tqwt_minValue_dec_10_mean', 'det_LT_entropy_shannon_10_coef_std', 'tqwt_medianValue_dec_31_std', 'std_8th_delta_delta_mean', 'tqwt_TKEO_mean_dec_21_mean', 'det_LT_entropy_log_6_coef_std', 'tqwt_TKEO_mean_dec_26_std', 'det_LT_entropy_log_7_coef_mean', 'tqwt_entropy_log_dec_12_mean', 'std_MFCC_3rd_coef_std', 'tqwt_medianValue_dec_1_std', 'tqwt_skewnessValue_dec_35_mean', 'tqwt_entropy_log_dec_11_mean', 'det_LT_entropy_shannon_10_coef_mean', 'mean_10th_delta_std', 'det_LT_entropy_log_1_coef_std', 'tqwt_maxValue_dec_12_mean', 'det_LT_entropy_log_2_coef_mean', 'f3_std', 'tqwt_medianValue_dec_22_std', 'tqwt_meanValue_dec_25_std', 'tqwt_maxValue_dec_11_mean', 'std_10th_delta_delta_std', 'det_LT_entropy_log_5_coef_mean', 'tqwt_skewnessValue_dec_17_std', 'tqwt_skewnessValue_dec_11_std', 'det_LT_entropy_shannon_9_coef_std', 'det_LT_entropy_shannon_7_coef_mean', 'mean_MFCC_2nd_coef_mean', 'tqwt_minValue_dec_12_mean', 'std_7th_delta_delta_mean', 'tqwt_entropy_log_dec_35_std', 'mean_2nd_delta_mean', 'det_LT_entropy_log_7_coef_std', 'det_LT_entropy_shannon_7_coef_std', 'IMF_SNR_entropy_mean', 'tqwt_entropy_shannon_dec_11_mean', 'tqwt_minValue_dec_13_mean', 'det_LT_entropy_log_5_coef_std', 'det_LT_entropy_log_1_coef_mean', 'det_LT_entropy_log_2_coef_std', 'tqwt_medianValue_dec_12_std', 'std_9th_delta_delta_mean', 'mean_9th_delta_delta_std', 'det_LT_entropy_shannon_1_coef_mean', 'det_LT_entropy_log_4_coef_std', 'det_LT_entropy_shannon_9_coef_mean', 'tqwt_entropy_log_dec_36_std', 'det_LT_entropy_log_8_coef_mean']\n"
     ]
    }
   ],
   "source": [
    "from SelectionType.EFSA.main import FeatureSelector\n",
    "\n",
    "# Initialize the Feature Selector\n",
    "selector = FeatureSelector(X_train, y_train)\n",
    "# Perform feature selection based on anova embedding and wrapper methods\n",
    "selected_features = selector.comprehensive_feature_selection(k_features=80) \n",
    "\n",
    "print(f\"\\nSelected Features: {selected_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classifier with selected features:\n",
      "Accuracy: 0.7647058823529411\n",
      "ROC AUC: 0.6396761133603239\n",
      "Confusion Matrix:\n",
      "[[ 5  8]\n",
      " [ 4 34]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.38      0.45        13\n",
      "           1       0.81      0.89      0.85        38\n",
      "\n",
      "    accuracy                           0.76        51\n",
      "   macro avg       0.68      0.64      0.65        51\n",
      "weighted avg       0.74      0.76      0.75        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train catboost using selected features\n",
    "cat_selected = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    loss_function='Logloss',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_selected.fit(X_train[selected_features], y_train)\n",
    "\n",
    "y_pred_selected_cat = cat_selected.predict(X_test[selected_features])\n",
    "\n",
    "# Evaluate\n",
    "print(\"CatBoost Classifier with selected features:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_selected_cat)}\")\n",
    "print(f\"ROC AUC: {roc_auc_score(y_test, y_pred_selected_cat)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_selected_cat)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_selected_cat)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training our catboost using the selected features\n",
    "\n",
    "cat_scratch_selected = CatBoostScratch(\n",
    "    n_estimators=1000,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "\n",
    "cat_scratch_selected.fit(X_train[selected_features], y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred_cat_scratch_selected = cat_scratch_selected.predict(X_test[selected_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix:\n",
      "[[ 5  8]\n",
      " [ 2 36]]\n",
      "\n",
      "Accuracy score:\n",
      "0.803921568627451\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.38      0.50        13\n",
      "           1       0.82      0.95      0.88        38\n",
      "\n",
      "    accuracy                           0.80        51\n",
      "   macro avg       0.77      0.67      0.69        51\n",
      "weighted avg       0.79      0.80      0.78        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_cat_scratch_selected))\n",
    "\n",
    "print(\"\\nAccuracy score:\")\n",
    "print(accuracy_score(y_test, y_pred_cat_scratch_selected))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred_cat_scratch_selected))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Method 3: Dimensionality Reduction  \n",
    "Principal Component Analysis (PCA) is used to reduce the dataset's dimensionality before applying the **CatBoost**.\n",
    "\n",
    "### Evaluation Metrics:  \n",
    "- Accuracy, Precision, Recall, F1-Score, ROC-AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bouma\\AppData\\Local\\Temp\\ipykernel_35160\\1472729090.py:34: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X_train_pca = X_train_pca.astype(float)\n",
      "C:\\Users\\bouma\\AppData\\Local\\Temp\\ipykernel_35160\\1472729090.py:35: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  X_test_pca = X_test_pca.astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost Classifier with PCA:\n",
      "Accuracy: 0.6862745098039216\n",
      "Confusion Matrix:\n",
      "[[ 8  5]\n",
      " [11 27]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.62      0.50        13\n",
      "           1       0.84      0.71      0.77        38\n",
      "\n",
      "    accuracy                           0.69        51\n",
      "   macro avg       0.63      0.66      0.64        51\n",
      "weighted avg       0.74      0.69      0.70        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from SelectionType.acp import PCAFromScratch\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# Splitting the data into train and test sets to prevent overfitting\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# Initialize SMOTE and PCA\n",
    "smote = SMOTE(random_state=42)\n",
    "pca = PCAFromScratch(n_components=80)  # Adjust number of components as needed\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Apply PCA to the scaled dataset\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "# Ensure data is float type\n",
    "X_train_pca = X_train_pca.astype(float)\n",
    "X_test_pca = X_test_pca.astype(float)\n",
    "\n",
    "# Initialize CatBoost Classifier with regularization and early stopping\n",
    "cat_pca = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=7,\n",
    "    loss_function='MultiClass',\n",
    "    early_stopping_rounds=50,  # Stop if no improvement for 50 iterations\n",
    "    use_best_model=True,  # Use the best model during training\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Train the CatBoost Classifier on PCA-transformed data\n",
    "cat_pca.fit(\n",
    "    X_train_pca, y_train_smote,\n",
    "    eval_set=(X_test_pca, y_test),\n",
    "    plot=False\n",
    ")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_cat_pca = cat_pca.predict(X_test_pca)\n",
    "\n",
    "print(\"CatBoost Classifier with PCA:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_cat_pca)}\")\n",
    "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred_cat_pca)}\")\n",
    "print(f\"Classification Report:\\n{classification_report(y_test, y_pred_cat_pca)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix:\n",
      "[[ 6  7]\n",
      " [10 28]]\n",
      "\n",
      "Accuracy score:\n",
      "0.6666666666666666\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.38      0.46      0.41        13\n",
      "           1       0.80      0.74      0.77        38\n",
      "\n",
      "    accuracy                           0.67        51\n",
      "   macro avg       0.59      0.60      0.59        51\n",
      "weighted avg       0.69      0.67      0.68        51\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_scratch_pca = CatBoostScratch(\n",
    "    n_estimators=1000,\n",
    "    max_depth=4,\n",
    "    learning_rate=0.1,\n",
    ")\n",
    "\n",
    "cat_scratch_pca.fit(X_train_pca, y_train_smote)\n",
    "\n",
    "# Predict\n",
    "y_pred_cat_scratch_pca = cat_scratch_pca.predict(X_test_pca)\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_cat_scratch_pca))\n",
    "\n",
    "print(\"\\nAccuracy score:\")\n",
    "print(accuracy_score(y_test, y_pred_cat_scratch_pca))\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred_cat_scratch_pca))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Comparative Analysis  \n",
    "- **Objective**: Compare the performance of the **CatBoost** across:  \n",
    "  1. All Features.  \n",
    "  2. Feature Selection.  \n",
    "  3. PCA.  \n",
    "- **Comparison Metrics**:  \n",
    "  - Performance (Accuracy, Precision, Recall, F1-Score, ROC-AUC).  \n",
    "  - Computational efficiency (Training and Testing Time).  \n",
    "- Present results as tables and visualizations (e.g., bar plots, ROC curves).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
